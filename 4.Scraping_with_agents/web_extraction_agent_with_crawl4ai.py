from textwrap import dedent
from typing import Dict, List, Optional, Any
import asyncio
import time
from datetime import datetime

from agno.agent import Agent
from agno.models.mistral import MistralChat
from pydantic import BaseModel, Field
from rich.pretty import pprint
from rich.console import Console
from crawl4ai import AsyncWebCrawler

from dotenv import load_dotenv
load_dotenv()

console = Console()

class ContentSection(BaseModel):
    heading: Optional[str] = Field(default=None, description="Section heading")
    content: str = Field(..., description="Section content text")
    subsections: Optional[List[str]] = Field(default=None, description="Subsections or bullet points")

class LinkInfo(BaseModel):
    url: str = Field(..., description="Link URL")
    text: str = Field(..., description="Link text/title")
    category: Optional[str] = Field(default=None, description="Link category (navigation, article, external, etc.)")

class ContactInfo(BaseModel):
    email: Optional[str] = Field(default=None, description="Email address")
    phone: Optional[str] = Field(default=None, description="Phone number")
    address: Optional[str] = Field(default=None, description="Physical address")
    
class ExtractionDiagnostics(BaseModel):
    """Informations de diagnostic pour l'extraction."""
    crawl_time_seconds: float = Field(..., description="Temps de crawling en secondes")
    processing_time_seconds: float = Field(..., description="Temps de traitement LLM en secondes")
    content_length: int = Field(..., description="Longueur du contenu brut")
    extraction_timestamp: str = Field(..., description="Timestamp de l'extraction")
    crawl4ai_version: str = Field(default="0.6.3", description="Version de Crawl4AI utilis√©e")
    success: bool = Field(..., description="Succ√®s de l'extraction")
    errors: Optional[List[str]] = Field(default=None, description="Erreurs rencontr√©es")

class PageInformation(BaseModel):
    """Mod√®le complet pour l'extraction de pages web."""
    
    # Informations de base
    url: str = Field(..., description="URL of the page")
    title: str = Field(..., description="Main page title")
    description: Optional[str] = Field(default=None, description="Meta description or page summary")
    
    # Contenu principal
    main_content: str = Field(..., description="Primary page content as clean text")
    content_sections: Optional[List[ContentSection]] = Field(default=None, description="Organized content sections")
    
    # Navigation et liens
    navigation_links: Optional[List[LinkInfo]] = Field(default=None, description="Main navigation links")
    article_links: Optional[List[LinkInfo]] = Field(default=None, description="Links to articles or main content")
    external_links: Optional[List[LinkInfo]] = Field(default=None, description="External links")
    
    # Contenu structur√©
    headlines: Optional[List[str]] = Field(default=None, description="Major headlines found on the page")
    categories: Optional[List[str]] = Field(default=None, description="Content categories or topics")
    dates: Optional[List[str]] = Field(default=None, description="Publication dates found")
    authors: Optional[List[str]] = Field(default=None, description="Author names found")
    
    # M√©tadonn√©es
    page_type: Optional[str] = Field(default=None, description="Type of page (news, business, blog, etc.)")
    language: Optional[str] = Field(default=None, description="Primary language")
    contact_info: Optional[ContactInfo] = Field(default=None, description="Contact information")
    social_media: Optional[Dict[str, str]] = Field(default=None, description="Social media links")
    
    # Donn√©es additionnelles
    metadata: Optional[Dict[str, Any]] = Field(default=None, description="Additional structured data")
    
    # Diagnostic
    diagnostics: ExtractionDiagnostics = Field(..., description="Extraction diagnostics")

# Agent d'extraction optimis√©
extraction_agent = Agent(
    model=MistralChat(id="mistral-large-2411"),
    instructions=dedent("""
        You are an expert web content analyzer specializing in comprehensive information extraction.

        **EXTRACTION MISSION:**
        Extract ALL meaningful information from the provided webpage content with maximum completeness and accuracy.

        **CORE EXTRACTION AREAS:**
        
        1. **Content Analysis:**
           - Main page title and meta description
           - Primary content (clean, readable text without navigation clutter)
           - All major headlines, article titles, and news items
           - Content organized into logical sections with headings
           
        2. **Link Classification:**
           - Navigation links (main menu, breadcrumbs)
           - Article/content links (links to individual articles or pages)
           - External links (pointing to other domains)
           - Include both URL and descriptive text for each link
           
        3. **Structured Data:**
           - Publication dates, timestamps
           - Author names and bylines
           - Content categories, tags, topics
           - Any structured data like prices, locations, events
           
        4. **Page Metadata:**
           - Page type classification (news, business, blog, e-commerce, etc.)
           - Primary language
           - Contact information if present
           - Social media links
           
        **QUALITY STANDARDS:**
        - Be comprehensive but precise - include everything meaningful
        - Clean and organize content for readability
        - Categorize links accurately
        - Extract dates in recognizable formats
        - Only include contact info if explicitly found
        - Set optional fields to null if no relevant data exists
        
        **CRITICAL RULES:**
        - Never fabricate information
        - If a field cannot be determined, set it to null
        - Prioritize accuracy over completeness
        - Clean text of navigation artifacts and ads
    """).strip(),
    response_model=PageInformation,
)

async def crawl_webpage(url: str) -> tuple[str, float]:
    """
    Crawl une page web avec Crawl4AI optimis√©.
    Retourne le contenu et le temps de crawling.
    """
    start_time = time.time()
    
    async with AsyncWebCrawler(
        verbose=True,
        headless=True,
    ) as crawler:
        result = await crawler.arun(
            url=url,
            # Configuration optimis√©e pour extraction compl√®te
            word_count_threshold=3,  # Inclusif pour capturer plus de contenu
            exclude_external_links=False,  # Garder tous les liens
            exclude_social_media_links=False,  # Garder les r√©seaux sociaux
            remove_overlay_elements=True,  # Supprimer popups et overlays
            process_iframes=True,  # Traiter les iframes
            remove_forms=False,  # Garder les formulaires pour l'analyse
            delay_before_return_html=2.0,  # Attendre le rendu JavaScript
            
            # Options avanc√©es
            css_selector=None,  # Pas de restriction de s√©lecteur
            screenshot=False,  # Pas besoin de screenshot
            pdf=False,  # Pas besoin de PDF
        )
        
        crawl_time = time.time() - start_time
        
        if result.success:
            return result.markdown, crawl_time
        else:
            raise Exception(f"Crawl4AI failed: {result.error_message}")

def extract_page_information(url: str) -> PageInformation:
    """
    Fonction principale d'extraction compl√®te et robuste.
    """
    console.print(f"üöÄ [bold blue]Extraction de:[/bold blue] {url}")
    
    errors = []
    
    try:
        # Phase 1: Crawling
        console.print("üì° [yellow]Phase 1:[/yellow] Crawling avec Crawl4AI...")
        raw_content, crawl_time = asyncio.run(crawl_webpage(url))
        
        console.print(f"‚úÖ [green]Crawl r√©ussi:[/green] {len(raw_content):,} caract√®res en {crawl_time:.2f}s")
        
        # Phase 2: Traitement LLM
        console.print("üß† [yellow]Phase 2:[/yellow] Traitement par LLM...")
        processing_start = time.time()
        
        # Limitation du contenu pour √©viter les timeouts
        content_for_llm = raw_content[:25000] if len(raw_content) > 25000 else raw_content
        if len(raw_content) > 25000:
            console.print(f"‚ö†Ô∏è  [orange]Contenu tronqu√©:[/orange] {len(raw_content):,} ‚Üí {len(content_for_llm):,} caract√®res")
        
        # Appel √† l'agent d'extraction
        extraction_prompt = f"""
Analyze and extract comprehensive information from this webpage.

URL: {url}
Content Length: {len(content_for_llm):,} characters

WEBPAGE CONTENT:
{content_for_llm}

Extract all meaningful information following the detailed guidelines provided in your instructions.
"""
        
        structured_data = extraction_agent.run(extraction_prompt)
        processing_time = time.time() - processing_start
        
        console.print(f"‚úÖ [green]Traitement r√©ussi[/green] en {processing_time:.2f}s")
        
        # Cr√©ation des diagnostics
        diagnostics = ExtractionDiagnostics(
            crawl_time_seconds=crawl_time,
            processing_time_seconds=processing_time,
            content_length=len(raw_content),
            extraction_timestamp=datetime.now().isoformat(),
            success=True,
            errors=errors if errors else None
        )
        
        # Ajout des diagnostics au r√©sultat
        result = structured_data.content
        result.diagnostics = diagnostics
        
        return result
        
    except Exception as e:
        error_msg = str(e)
        errors.append(error_msg)
        console.print(f"‚ùå [red]Erreur:[/red] {error_msg}")
        
        # Cr√©ation d'un r√©sultat d'erreur avec diagnostics
        diagnostics = ExtractionDiagnostics(
            crawl_time_seconds=0.0,
            processing_time_seconds=0.0,
            content_length=0,
            extraction_timestamp=datetime.now().isoformat(),
            success=False,
            errors=errors
        )
        
        return PageInformation(
            url=url,
            title="ERREUR D'EXTRACTION",
            main_content=f"Erreur lors de l'extraction: {error_msg}",
            diagnostics=diagnostics
        )

def print_extraction_summary(result: PageInformation):
    """Affiche un r√©sum√© d√©taill√© de l'extraction."""
    
    console.print("\n" + "="*80)
    console.print(f"üìä [bold cyan]R√âSUM√â D'EXTRACTION[/bold cyan]")
    console.print("="*80)
    
    # Informations de base
    console.print(f"üåê [bold]URL:[/bold] {result.url}")
    console.print(f"üì∞ [bold]Titre:[/bold] {result.title}")
    console.print(f"üè∑Ô∏è  [bold]Type:[/bold] {result.page_type or 'Non d√©termin√©'}")
    console.print(f"üåç [bold]Langue:[/bold] {result.language or 'Non d√©termin√©e'}")
    
    # Contenu extrait
    console.print(f"\nüìù [bold yellow]CONTENU EXTRAIT:[/bold yellow]")
    console.print(f"  ‚Ä¢ Contenu principal: {len(result.main_content):,} caract√®res")
    console.print(f"  ‚Ä¢ Headlines: {len(result.headlines or [])}")
    console.print(f"  ‚Ä¢ Sections: {len(result.content_sections or [])}")
    console.print(f"  ‚Ä¢ Dates trouv√©es: {len(result.dates or [])}")
    console.print(f"  ‚Ä¢ Auteurs: {len(result.authors or [])}")
    
    # Liens
    console.print(f"\nüîó [bold blue]LIENS EXTRAITS:[/bold blue]")
    console.print(f"  ‚Ä¢ Navigation: {len(result.navigation_links or [])}")
    console.print(f"  ‚Ä¢ Articles: {len(result.article_links or [])}")
    console.print(f"  ‚Ä¢ Externes: {len(result.external_links or [])}")
    
    # Diagnostics
    diag = result.diagnostics
    console.print(f"\n‚ö° [bold green]PERFORMANCE:[/bold green]")
    console.print(f"  ‚Ä¢ Crawling: {diag.crawl_time_seconds:.2f}s")
    console.print(f"  ‚Ä¢ Traitement LLM: {diag.processing_time_seconds:.2f}s")
    console.print(f"  ‚Ä¢ Total: {diag.crawl_time_seconds + diag.processing_time_seconds:.2f}s")
    console.print(f"  ‚Ä¢ Contenu brut: {diag.content_length:,} caract√®res")
    console.print(f"  ‚Ä¢ Succ√®s: {'‚úÖ' if diag.success else '‚ùå'}")
    
    if diag.errors:
        console.print(f"\n‚ö†Ô∏è  [bold red]ERREURS:[/bold red]")
        for error in diag.errors:
            console.print(f"  ‚Ä¢ {error}")
    
    # Aper√ßu du contenu
    if result.headlines:
        console.print(f"\nüì∞ [bold]PREMIERS HEADLINES:[/bold]")
        for i, headline in enumerate(result.headlines[:5], 1):
            console.print(f"  {i}. {headline}")
    
    console.print("\n" + "="*80)

def batch_extract_pages(urls: List[str]) -> Dict[str, PageInformation]:
    """Extraction en lot pour plusieurs URLs."""
    
    console.print(f"üöÄ [bold]Extraction en lot:[/bold] {len(urls)} URLs")
    
    results = {}
    
    for i, url in enumerate(urls, 1):
        console.print(f"\n[bold cyan]>>> {i}/{len(urls)}[/bold cyan]")
        
        try:
            result = extract_page_information(url)
            results[url] = result
            
            # R√©sum√© rapide
            status = "‚úÖ" if result.diagnostics.success else "‚ùå"
            headlines_count = len(result.headlines or [])
            console.print(f"{status} [green]{result.title}[/green] - {headlines_count} headlines")
            
        except Exception as e:
            console.print(f"‚ùå [red]Erreur pour {url}:[/red] {e}")
            results[url] = None
    
    # Statistiques finales
    successful = len([r for r in results.values() if r and r.diagnostics.success])
    console.print(f"\nüéØ [bold]R√©sultats:[/bold] {successful}/{len(urls)} extractions r√©ussies")
    
    return results

# Interface principale
def main():
    """Interface principale du script."""
    
    console.print("üï∑Ô∏è  [bold blue]EXTRACTEUR WEB CRAWL4AI + AGNO[/bold blue]")
    console.print("="*60)
    
    # URLs de test
    test_urls = [
        #"https://www.nbcnews.com/business",
        #"https://www.agno.com",
        #"https://github.com/unclecode/crawl4ai",
        #"https://techcrunch.com",
        "https://www.lemonde.fr/"
    ]
    
    # Test sur une URL
    console.print("\nüéØ [bold]Test sur une URL:[/bold]")
    result = extract_page_information(test_urls[0])
    print_extraction_summary(result)
    
    # Option: extraction d√©taill√©e
    console.print(f"\nüîç [bold]R√©sultat d√©taill√© disponible[/bold]")
    console.print("Pour voir le r√©sultat complet, d√©commentez la ligne ci-dessous:")
    # pprint(result)  # D√©commenter pour voir le r√©sultat complet
    
    # Test en lot
    console.print(f"\nüì¶ [bold]Test en lot sur {len(test_urls)} URLs:[/bold]")
    batch_results = batch_extract_pages(test_urls)
    
    return result, batch_results

if __name__ == "__main__":
    # Lancement du script
    single_result, batch_results = main()
    
    console.print("\n‚ú® [bold green]Extraction termin√©e![/bold green]")
    console.print("Les r√©sultats sont disponibles dans les variables 'single_result' et 'batch_results'")